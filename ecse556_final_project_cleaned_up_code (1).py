# -*- coding: utf-8 -*-
"""ECSE556 Final Project Cleaned up Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10A-wTHb9cb5K0JCeleHzX4XZCHrkVVYr

# Imports
"""

!pip install torch-geometric
!pip install sentence-transformers
!pip install openai==0.28

import pandas as pd
import sentence_transformers
import torch
from sentence_transformers import SentenceTransformer
from torch_geometric.transforms import RandomLinkSplit, ToUndirected
from torch.nn import Embedding
import openai
from torch.nn import Linear
import torch.nn.functional as F
from torch_geometric.nn import SAGEConv, to_hetero
import torch_geometric.transforms as T
import random
from torch_geometric.data import HeteroData
import matplotlib.pyplot as plt
from torch import Tensor
from torch_geometric.nn import SAGEConv, to_hetero
import torch.nn.functional as F
from torch.nn import Linear
from torch_geometric.nn import SAGEConv
import torch
from torch import Tensor
import torch
from torch_geometric.nn import GCNConv, MLP

"""# Preprocessing"""

############### Some helpher methods###############
def load_node_csv(path, index_col, encoders=None, **kwargs):
    df = pd.read_csv(path, index_col=index_col, **kwargs)
    mapping = {index: i for i, index in enumerate(df.index.unique())}

    x = None
    if encoders is not None:
        xs = [encoder(df[col]) for col, encoder in encoders.items()]
        x = torch.cat(xs, dim=-1)

    return x, mapping


def load_edge_csv(path, src_index_col, src_mapping, dst_index_col, dst_mapping,
                  encoders=None, **kwargs):
    df = pd.read_csv(path, **kwargs)

    src = [src_mapping[index] for index in df[src_index_col]]
    dst = [dst_mapping[index] for index in df[dst_index_col]]
    edge_index = torch.tensor([src, dst])

    edge_attr = None
    if encoders is not None:
        edge_attrs = [encoder(df[col]) for col, encoder in encoders.items()]
        edge_attr = torch.cat(edge_attrs, dim=-1)

    return edge_index, edge_attr

class SequenceEncoder:
    def __init__(self, model_name='all-MiniLM-L6-v2', device=None):
        self.device = device
        self.model = SentenceTransformer(model_name, device=device)

    @torch.no_grad()
    def __call__(self, df):
        x = self.model.encode(df.values, show_progress_bar=True,
                              convert_to_tensor=True, device=self.device)
        return x.cpu()

class IdentityEncoder:
    def __init__(self, dtype=None):
        self.dtype = dtype

    def __call__(self, df):
        return torch.from_numpy(df.values).view(-1, 1).to(self.dtype)

##################### Buidling the graph#####################

# Combining all nodes and all edges
disease_to_drug_path = "/content/D_Di_res.csv"
disease_to_symptom_path = "/content/Di_Sy_res.csv"
DSI_to_symptom_path = "/content/SDSI_Sy_res.csv"
DSI_to_TC_path = "/content/SDSI_TC_res.csv"

df_disease_to_drug = pd.read_csv(disease_to_drug_path)
df_disease_to_symptom = pd.read_csv(disease_to_symptom_path)
df_DSI_to_Symptom = pd.read_csv(DSI_to_symptom_path)
df_DSI_to_TC = pd.read_csv(DSI_to_TC_path)

#Renaming the columns so that all nodes are read with the same title :
df_DSI_to_Symptom = df_DSI_to_Symptom.rename(columns={'SDSI': 'DSI', 'DSI': 'DSI'})
df_DSI_to_TC = df_DSI_to_TC.rename(columns={'SDSI': 'DSI', 'DSI': 'DSI'})



#Starting with the Symptom nodes (node type 1)
_, Symptom_mapping_1 = load_node_csv(
    DSI_to_symptom_path, index_col='Symptom')
_, Symptom_mapping_2 = load_node_csv(
    disease_to_symptom_path, index_col='Symptom')
Symptom_mapping = {**Symptom_mapping_1, **Symptom_mapping_2}
reset_dict = {key: new_index for new_index, key in enumerate(Symptom_mapping.keys())}
Symptom_mapping = reset_dict
#print(f' We are expecting a length of 885 + 415 - 65 overlap  = {len(Symptom_mapping)}')


#Then the Disease nodes (node type 2)
_, Disease_mapping_1 = load_node_csv(
    disease_to_drug_path, index_col='Disease')
_, Disease_mapping_2 = load_node_csv(
    disease_to_symptom_path, index_col='Disease')
Disease_mapping = {**Disease_mapping_1, **Disease_mapping_2}
reset_dict = {key: new_index for new_index, key in enumerate(Disease_mapping.keys())}
Disease_mapping = reset_dict




#Then the DSI nodes (node type 3)
_, DSI_mapping_1 = load_node_csv(
    DSI_to_symptom_path, index_col='DSI')

_, DSI_mapping_2 = load_node_csv(
    DSI_to_TC_path, index_col='SDSI')
DSI_mapping = {**DSI_mapping_1, **DSI_mapping_2}
reset_dict = {key: new_index for new_index, key in enumerate(DSI_mapping.keys())}
DSI_mapping = reset_dict



#Then the TC nodes (node type 4)
_, TC_mapping = load_node_csv(
    DSI_to_TC_path, index_col='TC')



#Finally, the drug nodes (note type 5)
_, Drug_mapping = load_node_csv(
    disease_to_drug_path, index_col='Drug')


####### Visualizing the number of nodes per node types we have
print(f'We have {len(Disease_mapping)} Disease nodes')
print(f'We have {len(Symptom_mapping)} Symptom nodes')
print(f'We have {len(DSI_mapping)} DSI nodes')
print(f'We have {len(TC_mapping)} TC nodes')
print(f'We have {len(Drug_mapping)} Drug nodes')




###### Creating a HeteroData datastructure
data = HeteroData()

data['Symptom'].num_nodes = len(Symptom_mapping)  # Users do not have any features.
data['Disease'].num_nodes = len(Disease_mapping)
data['DSI'].num_nodes = len(DSI_mapping)
data['TC'].num_nodes = len(TC_mapping)
data['Drug'].num_nodes = len(Drug_mapping)

print(f' Our Heterogeneous graph looks like this : {data}')

############# Importing the edges, Initializing the nodes: ###############
# 4 Files of data that store edge information:

edge_index_Di_D, edge_label_Di_D = load_edge_csv(
    disease_to_drug_path,
    src_index_col='Drug',
    src_mapping=Drug_mapping,
    dst_index_col='Disease',
    dst_mapping=Disease_mapping,
    encoders={'Treats': IdentityEncoder(dtype=torch.long)},
)

edge_index_Di_S, edge_label_Di_S = load_edge_csv(
    disease_to_symptom_path,
    src_index_col='Disease',
    src_mapping=Disease_mapping,
    dst_index_col='Symptom',
    dst_mapping=Symptom_mapping,
    encoders={'Present': IdentityEncoder(dtype=torch.long)},
)

edge_index_DSI_S, edge_label_DSI_S = load_edge_csv(
    DSI_to_symptom_path,
    src_index_col='DSI',
    src_mapping=DSI_mapping,
    dst_index_col='Symptom',
    dst_mapping=Symptom_mapping,
    encoders={'has_adverse_reaction': IdentityEncoder(dtype=torch.long)},
)

edge_index_DSI_TC, edge_label_DSI_TC = load_edge_csv(
    DSI_to_TC_path,
    src_index_col='SDSI',
    src_mapping=DSI_mapping,
    dst_index_col='TC',
    dst_mapping=TC_mapping,
    encoders={'has_therapeutic_class': IdentityEncoder(dtype=torch.long)},
)


data['Disease', 'treats', 'Drug'].edge_index = edge_index_Di_D
data['Disease', 'treats', 'Drug'].edge_label = edge_label_Di_D

data['Disease', 'Present', 'Symptom'].edge_index = edge_index_Di_S
data['Disease', 'Present', 'Symptom'].edge_label = edge_label_Di_S

data['DSI', 'has_adverse_reaction', 'Symptom'].edge_index = edge_index_DSI_S
data['DSI', 'has_adverse_reaction', 'Symptom'].edge_label = edge_label_DSI_S

data['DSI', 'has_therapeutic_class', 'TC'].edge_index = edge_index_DSI_TC
data['DSI', 'has_therapeutic_class', 'TC'].edge_label = edge_label_DSI_TC


################################################################################
#Option 1 : Generate some initial embeddings for each node:
################################################################################



# Embedding dimension
embedding_dim = 12  # Choose the embedding dimension

# Create Embedding layers for each type of node
symptom_embedding = Embedding(len(Symptom_mapping), embedding_dim)
disease_embedding = Embedding(len(Disease_mapping), embedding_dim)
dsi_embedding = Embedding(len(DSI_mapping), embedding_dim)
tc_embedding = Embedding(len(TC_mapping), embedding_dim)
drug_embedding = Embedding(len(Drug_mapping), embedding_dim)

# Add node embeddings
data['Symptom'].x = symptom_embedding.weight
data['Disease'].x = disease_embedding.weight
data['DSI'].x = dsi_embedding.weight
data['TC'].x = tc_embedding.weight
data['Drug'].x = drug_embedding.weight

'''
################################################################################
#Option 2 :  torch ones to initialize representation of all nodes
################################################################################

#data['Symptom'].x = torch.ones(len(Symptom_mapping), embedding_dim)
#data['Disease'].x = torch.ones(len(Disease_mapping), embedding_dim)
#data['DSI'].x = torch.ones(len(DSI_mapping), embedding_dim)
#data['TC'].x = torch.ones(len(TC_mapping), embedding_dim)
#data['Drug'].x = torch.ones(len(Drug_mapping), embedding_dim)
'''


# Print sizes of node embeddings to verify
print(f"Symptom node features: {data['Symptom'].x.shape}")
print(f"Disease node features: {data['Disease'].x.shape}")
print(f"DSI node features: {data['DSI'].x.shape}")
print(f"TC node features: {data['TC'].x.shape}")
print(f"Drug node features: {data['Drug'].x.shape}")

# Print the first few rows of the embeddings for each node type
print("Symptom node embeddings:")
print(data['Symptom'].x[:5])

print("Disease node embeddings:")
print(data['Disease'].x[:5])

print("DSI node embeddings:")
print(data['DSI'].x[:5])

print("TC node embeddings:")
print(data['TC'].x[:5])

print("Drug node embeddings:")
print(data['Drug'].x[:5])

########## Some summary information about our heterogeneous graph : ##############
print(f'Our data looks like this : {data}')
print(f'data edge types = {data.edge_types}')
print(f'number of nodes {data.num_nodes}')
print(f'number of edges {data.num_edges}')

"""## Some visualization on our graph"""

import matplotlib.pyplot as plt
import networkx as nx
from torch_geometric.data import HeteroData
from torch_geometric.utils import to_networkx


data_networkx = to_networkx(data, node_attrs=['x'], edge_attrs=['edge_label'])

import random
import matplotlib.pyplot as plt

# Sample 10% of the nodes
num_nodes = len(data_networkx.nodes)
sampled_nodes = random.sample(data_networkx.nodes, int(0.1 * num_nodes))

# Create a subgraph with only the sampled nodes
subgraph = data_networkx.subgraph(sampled_nodes).copy()

# Define node and edge type colors
node_colors = {
    'Symptom': 'red',
    'Disease': 'blue',
    'DSI': 'green',
    'TC': 'purple',
    'Drug': 'orange',
}

edge_colors = {
    ('Disease', 'treats', 'Drug'): 'cyan',
    ('Disease', 'Present', 'Symptom'): 'pink',
    ('DSI', 'has_adverse_reaction', 'Symptom'): 'yellow',
    ('DSI', 'has_therapeutic_class', 'TC'): 'gray',
}

# Create color attributes for nodes
color_map = []
for node, node_data in subgraph.nodes(data=True):
    color_map.append(node_colors.get(node_data.get('type', 'black')))  # Default to black if 'type' is missing

edge_color_map = []
for u, v, edge_data in subgraph.edges(data=True):
    edge_type = edge_data.get('edge_type', None)  # Ensure correct key access
    edge_color_map.append(edge_colors.get(edge_type, 'black'))  # Default to black if undefined

# Position nodes using a layout (e.g., spring layout)
pos = nx.spring_layout(subgraph)

plt.figure(figsize=(100, 100))


# Draw nodes with colors
nx.draw_networkx_nodes(subgraph, pos, node_color=color_map, node_size=50)

# Draw edges with colors
nx.draw_networkx_edges(subgraph, pos, edge_color=edge_color_map, alpha=0.5)

# Optionally draw labels
nx.draw_networkx_labels(subgraph, pos, font_size=6)

# Display the plot

plt.title("Heterogeneous Graph Visualization (10% Node Sampling)")
plt.axis("off")

# Save the plot as a PDF
plt.savefig("heterogeneous_graph_visualization.pdf", format="pdf", bbox_inches="tight")


plt.show()

"""# Start of training (EdgePreditctor (GCN))

"""

################# Spliting data into training, validation, and test edges


# 2. Perform a link-level split into training, validation, and test edges.
transform = RandomLinkSplit(
    num_val=0.1, #Amount of total number of edges considered for validation
    num_test=0.1, #Amount of total number of edges considered for testing
    is_undirected=True, #Checks if the edges are undirected or if they are (here (at least for now), we consider that our graph is indeed undirected)
    split_labels=False, # If set to True, will split positive and negative labels and save them in distinct attributes "pos_edge_label" and "neg_edge_label", respectively. (default: False)
    disjoint_train_ratio=0.5, #Percentage of edges not used for message passing (i.e. for supervision)
    #We can therefore consider that we leave some edges for ground truth
    add_negative_train_samples=True, # shold remain True+. Whether the negative edges are in the dataset from before of generated from the fly
    neg_sampling_ratio=1, #ii.e. the rtio of sampled negative edges to the number of positive edges
    edge_types=[('DSI', 'has_therapeutic_class', 'TC')], #Only selecting one edge type in this implementation
    rev_edge_types=[('TC', 'has_therapeutic_class', 'DSI')] #optional : populating the rev_edge_type attribute altough our graph is undirected and should technically consider reverse nodes as the same as the original nodes.
)
train_data, val_data, test_data = transform(data)
print(f' The train_data looks like: {train_data}')
print("_________________________________________________")
print(f' The val_data looks like: {val_data}')
print("_________________________________________________")
print(f' The test_data looks like: {test_data}')

"""## Some visualization of nodes and edges"""

import torch
import matplotlib.pyplot as plt

def map_indices_to_names(indices, mapping_inv):
    return [mapping_inv.get(i, f"Unknown_{i}") for i in indices]

def invert_mapping(mapping):
    return {v: k for k, v in mapping.items()}

# Invert the mappings
Drug_mapping_inv = invert_mapping(Drug_mapping)
TC_mapping_inv = invert_mapping(TC_mapping)
Disease_mapping_inv = invert_mapping(Disease_mapping)
Symptom_mapping_inv = invert_mapping(Symptom_mapping)
DSI_mapping_inv = invert_mapping(DSI_mapping)

# Dictionary to store the top k nodes and their counts for each edge type
top_connected_nodes = {}

# Iterate through all edge types in the HeteroData object
for edge_type in data.edge_types:
    src_type, _, tgt_type = edge_type
    edge_index = data[edge_type].edge_index

    # Combine source and target nodes to calculate total degrees
    combined_degrees = torch.bincount(
        torch.cat([edge_index[0], edge_index[1]]),
        minlength=max(data[src_type].num_nodes, data[tgt_type].num_nodes)
    )

    # Get the top k most connected nodes
    top_count, top_indices = torch.topk(combined_degrees, k=10)

    # Determine the mapping to use for node names
    node_type = src_type if src_type == tgt_type else "Combined"
    if node_type == "Drug":
        node_names = map_indices_to_names(top_indices.tolist(), Drug_mapping_inv)
    elif node_type == "TC":
        node_names = map_indices_to_names(top_indices.tolist(), TC_mapping_inv)
    elif node_type == "Disease":
        node_names = map_indices_to_names(top_indices.tolist(), Disease_mapping_inv)
    elif node_type == "Symptom":
        node_names = map_indices_to_names(top_indices.tolist(), Symptom_mapping_inv)
    elif node_type == "DSI":
        node_names = map_indices_to_names(top_indices.tolist(), DSI_mapping_inv)
    else:
        node_names = [f"Node_{i}" for i in top_indices.tolist()]

    # Store results for visualization
    top_connected_nodes[edge_type] = (node_names, top_count.tolist())

    # Plot histogram
    plt.figure(figsize=(10, 5))
    plt.bar(node_names, top_count.tolist(), color='skyblue')
    plt.title(f"Top 10 Most Connected Nodes for Edge Type {edge_type}")
    plt.xlabel("Node Name")
    plt.ylabel("Connection Count")
    plt.xticks(rotation=45, ha="right")
    plt.tight_layout()
    plt.show()

# Print results
for edge_type, (node_names, counts) in top_connected_nodes.items():
    print(f"\nTop 10 most connected nodes for edge type {edge_type}:")
    for rank, (name, count) in enumerate(zip(node_names, counts), 1):
        print(f"{rank}. Node {name} appears {count} times")

############ Some visualization of edges
import random
import torch
from torch_geometric.data import HeteroData

def visualize_edges_with_labels(hetero_data, edge_type, sample_size=10):
    """
    Visualizes a sample of edges with labels for a given edge type in the HeteroData object.

    Args:
        hetero_data (HeteroData): The PyG HeteroData object.
        edge_type (tuple): The edge type (source, relation, target) to visualize.
        sample_size (int): Number of edges to sample and display.

    Returns:
        None: Prints the sampled edges with their labels.
    """
    edge_index = hetero_data[edge_type].edge_index
    edge_label = hetero_data[edge_type].edge_label if 'edge_label' in hetero_data[edge_type] else None
    num_edges = edge_index.size(1)

    # Select a random sample of edges
    sampled_indices = random.sample(range(num_edges), min(sample_size, num_edges))
    sampled_edges = edge_index[:, sampled_indices]

    # Get corresponding edge labels (if available)
    sampled_labels = edge_label[sampled_indices].tolist() if edge_label is not None else None

    # Convert edge indices to list of node pairs
    source_nodes = sampled_edges[0].tolist()
    target_nodes = sampled_edges[1].tolist()

    # Print sampled edges with their labels
    print(f"Sample of {len(sampled_indices)} edges for {edge_type}:")
    for i, (src, tgt) in enumerate(zip(source_nodes, target_nodes)):
        label_info = f" (Label: {sampled_labels[i]})" if sampled_labels is not None else ""
        print(f"  Edge {i + 1}: {edge_type[0]} Node {src} --> {edge_type[2]} Node {tgt}{label_info}")

# Example usage

edge_type = ("DSI", "has_therapeutic_class", "TC")
print(f'Some sample visualization of the "DSI", "has_therapeutic_class", "TC" edge for the Test data')
visualize_edges_with_labels(data, edge_type)

"""## Models"""

class EdgePredictor(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels):
        super().__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, hidden_channels)
        #self.mlp = MLP([hidden_channels, 1], act='relu', act_first=True)

    def forward(self, x, edge_index):
        # GNN to compute node embeddings
        x = self.conv1(x, edge_index).relu()
        x = self.conv2(x, edge_index)
        return x  # Node embeddings

# Change the output with a sigmoid so that the output is between 0 and 1
    def predict(self, x, edge_index): #here edge_index is the same as edge_level_index
        # Predict edges using node embeddings
        edge_features = x[edge_index[0]] * x[edge_index[1]]
        logits = self.mlp(edge_features).squeeze(-1)  # Raw scores (logits)
        edge_scores = torch.sigmoid(logits)  # Bound scores between 0 and 1
        return edge_scores #Outputs the scores that we have bounded between 0 and 1 (as some logits would diverge)

import torch
from torch.nn import Module
from torch_geometric.nn import SAGEConv
from torch_geometric.nn.models import MLP

class EdgePredictor2(Module):
    def __init__(self, in_channels, hidden_channels, num_layers=10):
        super().__init__()
        self.num_layers = num_layers

        # Create stacked SAGEConv layers dynamically
        self.convs = torch.nn.ModuleList()
        self.convs.append(SAGEConv(in_channels, hidden_channels))
        for _ in range(num_layers - 1):
            self.convs.append(SAGEConv(hidden_channels, hidden_channels))

        # MLP for edge prediction
        self.mlp = MLP([hidden_channels, hidden_channels // 2, 1], act='Leaky relu', act_first=True)

    def forward(self, x, edge_index):
        # Compute node embeddings with multiple GraphSAGE layers
        for conv in self.convs[:-1]:
            x = conv(x, edge_index).relu()
        x = self.convs[-1](x, edge_index)  # Final layer (no activation to preserve embedding structure)
        return x  # Node embeddings

    def predict(self, x, edge_index):
        # Compute edge features using Hadamard product
        src, dest = edge_index
        edge_features = x[src] * x[dest]  # Element-wise product of node embeddings
        logits = self.mlp(edge_features).squeeze(-1)  # Compute edge logits
        edge_scores = torch.sigmoid(logits)  # Map logits to [0, 1]
        return edge_scores

"""## Training Loop"""

########################################################################################################
#Calculating all node embeddings during message passing
########################################################################################################





import torch.nn.functional as F

# Initialize the model, optimizer, and loss function
model1 = EdgePredictor(in_channels=data['DSI'].x.shape[1], hidden_channels=12) #Change number of hidden channels (i.e. hidden layers)
optimizer = torch.optim.Adam(model1.parameters(), lr=0.01)
criterion = torch.nn.BCEWithLogitsLoss()
Train_loss = []
Val_loss = []
different_means_of_predictions = []



previous_node_embeddings = None
# Training loop
for epoch in range(1000):  # Adjust epochs as needed
    model1.train()
    optimizer.zero_grad()
        # Accessing node features for each node type
    symptom_nodes = train_data['Symptom'].x  # Shape: [1235, 6]
    disease_nodes = train_data['Disease'].x  # Shape: [8148, 6]
    dsi_nodes = train_data['DSI'].x  # Shape: [752, 6]
    tc_nodes = train_data['TC'].x  # Shape: [587, 6]
    drug_nodes = train_data['Drug'].x  # Shape: [20123, 6]
    concat_nodes = torch.cat([tc_nodes, disease_nodes, dsi_nodes, symptom_nodes, drug_nodes], dim=0) #Because all nodes have a similar embedding, we can concatenate them


    # Forward pass on train_data

    edge_index = train_data['DSI', 'has_therapeutic_class', 'TC'].edge_index
    edge_label_index = train_data['DSI', 'has_therapeutic_class', 'TC'].edge_label_index
    edge_label = (train_data['DSI', 'has_therapeutic_class', 'TC'].edge_label)//2 #this is your ground truth

    #updatign nodes so that our algorithm actually learns something
    if previous_node_embeddings is not None:
        x = previous_node_embeddings  # Use the embeddings from the previous iteration
    else:
        x = concat_nodes # Node features only starting here (just in case the edge is read DC --> TC although it should technically not matter?)

    # Compute node embeddings and predict edge probabilities
    node_embeddings = model1(x, edge_index)
    previous_node_embeddings = node_embeddings.detach()

    #print(f'shape of node_embeddings = {node_embeddings.shape}')

    edge_predictions = model1.predict(node_embeddings, edge_label_index)
    tensor_mean = torch.mean(edge_predictions)
    binary_prediciton_tensor = torch.where(edge_predictions > tensor_mean, 1, 0) #We can investigate different thresholds
    different_means_of_predictions.append(tensor_mean)



    # Compute loss and backpropagate
    loss = F.binary_cross_entropy(edge_predictions, edge_label.view(-1).float())
    Train_loss.append(loss.item())
    loss.backward()
    optimizer.step()

    # Validation step
    model1.eval()
    with torch.no_grad():
        val_edge_index = val_data['DSI', 'has_therapeutic_class', 'TC'].edge_label_index
        val_edge_label = val_data['DSI', 'has_therapeutic_class', 'TC'].edge_label//2

        val_predictions = model1.predict(node_embeddings, val_edge_index)
        val_mean = torch.mean(val_predictions)
        val_binary_prediciton_tensor = torch.where(val_predictions > val_mean, 1, 0)

        val_loss =  F.binary_cross_entropy(val_predictions, val_edge_label.view(-1).float())
        Val_loss.append(val_loss.item())


    print(f"Epoch {epoch}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}")

import matplotlib.pyplot as plt
values = torch.tensor(different_means_of_predictions, requires_grad=True).detach().numpy()
# Assuming 'different_means_of_predictions' is a list of means from your training loop
plt.figure(figsize=(10, 6))
plt.plot(values, label='Mean of Predictions')
plt.xlabel('Epoch')
plt.ylabel('Mean')
plt.title('Mean of Predictions at Each Epoch')
plt.legend()
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, auc

# Switch model to evaluation mode
model1.eval()
with torch.no_grad():
    # Retrieve the test data
    test_edge_index = test_data['DSI', 'has_therapeutic_class', 'TC'].edge_label_index
    test_edge_label = test_data['DSI', 'has_therapeutic_class', 'TC'].edge_label
    print(f'shape of test_edge_index = {test_edge_index.shape}')
    print(f'shape of test_edge_label = {test_edge_label.shape}')


    # Make predictions using the model
    test_logits = model1.predict(node_embeddings, test_edge_index)
    print(f'test_logits.shape - {test_logits.shape}')

    test_labels = test_edge_label.cpu().numpy()//2


    # Compute ROC-AUC
    auc_score = roc_auc_score(test_labels, test_logits)

    print(f"Test ROC-AUC: {auc_score:.4f}")

    # Calculate ROC Curve
    fpr, tpr, roc_thresholds = roc_curve(test_labels, test_logits)

    # Calculate Precision-Recall Curve
    precision, recall, pr_thresholds = precision_recall_curve(test_labels, test_logits)

    # Calculate the area under the precision-recall curve (PR-AUC)
    pr_auc = auc(recall, precision)
    print(f"Test PR-AUC: {pr_auc:.4f}")

# Plot ROC Curve
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(fpr, tpr, color='b', label=f'ROC curve (AUC = {auc_score:.4f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')

# Plot Precision-Recall Curve
plt.subplot(1, 2, 2)
plt.plot(recall, precision, color='b', label=f'PR curve (AUC = {pr_auc:.4f})')
plt.title('Precision-Recall Curve')
plt.xlabel('Recall')

plt.ylabel('Precision')
plt.legend(loc='lower left')

# Display the plots
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Assuming test_logits and test_labels are defined from your previous code
plt.figure(figsize=(10, 6))
plt.plot(test_logits.cpu().numpy(), label='Test predictions')
plt.plot(test_labels, label='Test Labels', linestyle='--')  # Use linestyle to differentiate
plt.xlabel('Data Point Index')
plt.ylabel('Value')
plt.title('Test Logits vs. Test Labels')
plt.legend()
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, precision_recall_curve


# Plot the loss curves
plt.figure(figsize=(10, 5))
plt.plot(Train_loss, label='Train Loss')
plt.plot(Val_loss, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

"""# Start of training EdgePredictor2 (GraphSage)

"""

################# Spliting data into training, validation, and test edges


transform = RandomLinkSplit(
    num_val=0.1, #Amount of total number of edges considered for validation
    num_test=0.1, #Amount of total number of edges considered for testing
    is_undirected=True, #Checks if the edges are undirected or if they are (here (at least for now), we consider that our graph is indeed undirected)
    split_labels=False, # If set to True, will split positive and negative labels and save them in distinct attributes "pos_edge_label" and "neg_edge_label", respectively. (default: False)
    disjoint_train_ratio=0.1, #Percentage of edges not used for message passing (i.e. for supervision)
    #We can therefore consider that we leave some edges for ground truth
    add_negative_train_samples=True, # shold remain True+. Whether the negative edges are in the dataset from before of generated from the fly
    neg_sampling_ratio=1, #ii.e. the rtio of sampled negative edges to the number of positive edges
    edge_types=[('DSI', 'has_therapeutic_class', 'TC')], #Only selecting one edge type in this implementation
    rev_edge_types=[('TC', 'has_therapeutic_class', 'DSI')] #optional : populating the rev_edge_type attribute altough our graph is undirected and should technically consider reverse nodes as the same as the original nodes.
)
train_data, val_data, test_data = transform(data)
print(f' The train_data looks like: {train_data}')
print("_________________________________________________")
print(f' The val_data looks like: {val_data}')
print("_________________________________________________")
print(f' The test_data looks like: {test_data}')

"""## Some visualization of nodes and edges"""

import torch
import matplotlib.pyplot as plt

def map_indices_to_names(indices, mapping_inv):
    return [mapping_inv.get(i, f"Unknown_{i}") for i in indices]

def invert_mapping(mapping):
    return {v: k for k, v in mapping.items()}

# Invert the mappings
Drug_mapping_inv = invert_mapping(Drug_mapping)
TC_mapping_inv = invert_mapping(TC_mapping)
Disease_mapping_inv = invert_mapping(Disease_mapping)
Symptom_mapping_inv = invert_mapping(Symptom_mapping)
DSI_mapping_inv = invert_mapping(DSI_mapping)

# Dictionary to store the top k nodes and their counts for each edge type
top_connected_nodes = {}

# Iterate through all edge types in the HeteroData object
for edge_type in data.edge_types:
    src_type, _, tgt_type = edge_type
    edge_index = data[edge_type].edge_index

    # Combine source and target nodes to calculate total degrees
    combined_degrees = torch.bincount(
        torch.cat([edge_index[0], edge_index[1]]),
        minlength=max(data[src_type].num_nodes, data[tgt_type].num_nodes)
    )

    # Get the top k most connected nodes
    top_count, top_indices = torch.topk(combined_degrees, k=100)

    # Determine the mapping to use for node names
    node_type = src_type if src_type == tgt_type else "Combined"
    if node_type == "Drug":
        node_names = map_indices_to_names(top_indices.tolist(), Drug_mapping_inv)
    elif node_type == "TC":
        node_names = map_indices_to_names(top_indices.tolist(), TC_mapping_inv)
    elif node_type == "Disease":
        node_names = map_indices_to_names(top_indices.tolist(), Disease_mapping_inv)
    elif node_type == "Symptom":
        node_names = map_indices_to_names(top_indices.tolist(), Symptom_mapping_inv)
    elif node_type == "DSI":
        node_names = map_indices_to_names(top_indices.tolist(), DSI_mapping_inv)
    else:
        node_names = [f"Node_{i}" for i in top_indices.tolist()]

    # Store results for visualization
    top_connected_nodes[edge_type] = (node_names, top_count.tolist())

    # Plot histogram
    plt.figure(figsize=(10, 5))
    plt.bar(node_names, top_count.tolist(), color='skyblue')
    plt.title(f"Top 10 Most Connected Nodes for Edge Type {edge_type}")
    plt.xlabel("Node Name")
    plt.ylabel("Connection Count")
    plt.xticks(rotation=45, ha="right")
    plt.tight_layout()
    plt.show()

# Print results
for edge_type, (node_names, counts) in top_connected_nodes.items():
    print(f"\nTop 10 most connected nodes for edge type {edge_type}:")
    for rank, (name, count) in enumerate(zip(node_names, counts), 1):
        print(f"{rank}. Node {name} appears {count} times")

############ Some visualization of edges
import random
import torch
from torch_geometric.data import HeteroData

def visualize_edges_with_labels(hetero_data, edge_type, sample_size=10):
    """
    Visualizes a sample of edges with labels for a given edge type in the HeteroData object.

    Args:
        hetero_data (HeteroData): The PyG HeteroData object.
        edge_type (tuple): The edge type (source, relation, target) to visualize.
        sample_size (int): Number of edges to sample and display.

    Returns:
        None: Prints the sampled edges with their labels.
    """
    edge_index = hetero_data[edge_type].edge_index
    edge_label = hetero_data[edge_type].edge_label if 'edge_label' in hetero_data[edge_type] else None
    num_edges = edge_index.size(1)

    # Select a random sample of edges
    sampled_indices = random.sample(range(num_edges), min(sample_size, num_edges))
    sampled_edges = edge_index[:, sampled_indices]

    # Get corresponding edge labels (if available)
    sampled_labels = edge_label[sampled_indices].tolist() if edge_label is not None else None

    # Convert edge indices to list of node pairs
    source_nodes = sampled_edges[0].tolist()
    target_nodes = sampled_edges[1].tolist()

    # Print sampled edges with their labels
    print(f"Sample of {len(sampled_indices)} edges for {edge_type}:")
    for i, (src, tgt) in enumerate(zip(source_nodes, target_nodes)):
        label_info = f" (Label: {sampled_labels[i]})" if sampled_labels is not None else ""
        print(f"  Edge {i + 1}: {edge_type[0]} Node {src} --> {edge_type[2]} Node {tgt}{label_info}")

# Example usage

edge_type = ("DSI", "has_therapeutic_class", "TC")
print(f'Some sample visualization of the "DSI", "has_therapeutic_class", "TC" edge for the Test data')
visualize_edges_with_labels(data, edge_type)

"""## Models"""

class EdgePredictor(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels):
        super().__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, hidden_channels)
        self.mlp = MLP([hidden_channels, 1], act='relu', act_first=True)

    def forward(self, x, edge_index):
        # GNN to compute node embeddings
        x = self.conv1(x, edge_index).relu()
        x = self.conv2(x, edge_index)
        return x  # Node embeddings

# Change the output with a sigmoid so that the output is between 0 and 1
    def predict(self, x, edge_index): #here edge_index is the same as edge_level_index
        # Predict edges using node embeddings
        edge_features = x[edge_index[0]] * x[edge_index[1]]
        logits = self.mlp(edge_features).squeeze(-1)  # Raw scores (logits)
        edge_scores = torch.sigmoid(logits)  # Bound scores between 0 and 1
        return edge_scores #Outputs the scores that we have bounded between 0 and 1 (as some logits would diverge)

import torch
from torch.nn import Module
from torch_geometric.nn import SAGEConv
from torch_geometric.nn.models import MLP
from torch_geometric.nn import GATConv


class EdgePredictor2(Module):
    def __init__(self, in_channels, hidden_channels, num_layers=2):
        super().__init__()
        self.num_layers = num_layers

        # Create stacked SAGEConv layers dynamically
        self.convs = torch.nn.ModuleList()
        self.convs.append(SAGEConv(in_channels, hidden_channels))
        for _ in range(num_layers - 1):
            self.convs.append(SAGEConv(hidden_channels, hidden_channels, aggr = "max", normalize = True))

        # MLP for edge prediction
        self.mlp = MLP([hidden_channels, hidden_channels // 2, 1], act='relu', act_first=True)

    def forward(self, x, edge_index):
        # Compute node embeddings with multiple GraphSAGE layers
        for conv in self.convs[:-1]:
            x = conv(x, edge_index).relu()
        x = self.convs[-1](x, edge_index)  # Final layer (no activation to preserve embedding structure)
        return x  # Node embeddings

    def predict(self, x, edge_index):
        # Compute edge features using Hadamard product
        src, dest = edge_index
        edge_features = x[src] * x[dest]  # Element-wise product of node embeddings
        logits = self.mlp(edge_features).squeeze(-1)  # Compute edge logits
        edge_scores = torch.sigmoid(logits)  # Map logits to [0, 1]
        return edge_scores


class EdgePredictor3(Module):
    def __init__(self, in_channels, hidden_channels, num_layers=2):
        super().__init__()
        self.num_layers = num_layers

        # Create stacked SAGEConv layers dynamically
        self.convs = torch.nn.ModuleList()
        self.convs.append(SAGEConv(in_channels, hidden_channels))
        for _ in range(num_layers - 1):
            self.convs.append(GATConv(hidden_channels, hidden_channels, heads=2, concat=False))

        # MLP for edge prediction
        self.mlp = MLP([hidden_channels, hidden_channels // 2, 1], act='relu', act_first=True)

    def forward(self, x, edge_index):
        # Compute node embeddings with multiple GraphSAGE layers
        for conv in self.convs[:-1]:
            x = conv(x, edge_index).relu()
        x = self.convs[-1](x, edge_index)  # Final layer (no activation to preserve embedding structure)
        return x  # Node embeddings

    def predict(self, x, edge_index):
        # Compute edge features using Hadamard product
        src, dest = edge_index
        edge_features = x[src] * x[dest]  # Element-wise product of node embeddings
        logits = self.mlp(edge_features).squeeze(-1)  # Compute edge logits
        edge_scores = torch.sigmoid(logits)  # Map logits to [0, 1]
        return edge_scores

"""## Training Loop"""

########################################################################################################
#Calculating all node embeddings during message passing
########################################################################################################





import torch.nn.functional as F

# Initialize the model, optimizer, and loss function
model2 = EdgePredictor2(in_channels=data['DSI'].x.shape[1], hidden_channels=12, num_layers = 3) #Change number of hidden channels (i.e. hidden layers)
optimizer = torch.optim.Adam(model2.parameters(), lr=0.01)
criterion = torch.nn.BCEWithLogitsLoss()
Train_loss = []
Val_loss = []
different_means_of_predictions = []



previous_node_embeddings = None
# Training loop
for epoch in range(250):  # Adjust epochs as needed
    model2.train()
    optimizer.zero_grad()
        # Accessing node features for each node type
    symptom_nodes = train_data['Symptom'].x  # Shape: [1235, 6]
    disease_nodes = train_data['Disease'].x  # Shape: [8148, 6]
    dsi_nodes = train_data['DSI'].x  # Shape: [752, 6]
    tc_nodes = train_data['TC'].x  # Shape: [587, 6]
    drug_nodes = train_data['Drug'].x  # Shape: [20123, 6]
    concat_nodes = torch.cat([tc_nodes, disease_nodes, dsi_nodes, symptom_nodes, drug_nodes], dim=0) #Because all nodes have a similar embedding, we can concatenate them


    # Forward pass on train_data

    edge_index = train_data['DSI', 'has_therapeutic_class', 'TC'].edge_index
    edge_label_index = train_data['DSI', 'has_therapeutic_class', 'TC'].edge_label_index
    edge_label = (train_data['DSI', 'has_therapeutic_class', 'TC'].edge_label)//2 #this is your ground truth

    #updatign nodes so that our algorithm actually learns something
    if previous_node_embeddings is not None:
        x = previous_node_embeddings  # Use the embeddings from the previous iteration
    else:
        x = concat_nodes # Node features only starting here (just in case the edge is read DC --> TC although it should technically not matter?)

    # Compute node embeddings and predict edge probabilities
    node_embeddings = model2(x, edge_label_index)
    previous_node_embeddings = node_embeddings.detach()

    #print(f'shape of node_embeddings = {node_embeddings.shape}')

    edge_predictions = model2.predict(node_embeddings, edge_label_index)
    tensor_mean = torch.mean(edge_predictions)
    binary_prediciton_tensor = torch.where(edge_predictions > tensor_mean, 1, 0) #We can investigate different thresholds
    different_means_of_predictions.append(tensor_mean)



    # Compute loss and backpropagate
    loss = F.binary_cross_entropy(edge_predictions, edge_label.view(-1).float())
    Train_loss.append(loss.item())
    loss.backward()
    optimizer.step()

    # Validation step
    model2.eval()
    with torch.no_grad():
        val_edge_index = val_data['DSI', 'has_therapeutic_class', 'TC'].edge_label_index
        val_edge_label = val_data['DSI', 'has_therapeutic_class', 'TC'].edge_label//2

        val_predictions = model2.predict(node_embeddings, val_edge_index)
        val_mean = torch.mean(val_predictions)
        val_binary_prediciton_tensor = torch.where(val_predictions > val_mean, 1, 0)

        val_loss =  F.binary_cross_entropy(val_predictions, val_edge_label.view(-1).float())
        Val_loss.append(val_loss.item())


    print(f"Epoch {epoch}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}")

import matplotlib.pyplot as plt
values = torch.tensor(different_means_of_predictions, requires_grad=True).detach().numpy()
# Assuming 'different_means_of_predictions' is a list of means from your training loop
plt.figure(figsize=(10, 6))
plt.plot(values, label='Mean of Predictions')
plt.xlabel('Epoch')
plt.ylabel('Mean')
plt.title('Mean of Predictions at Each Epoch')
plt.legend()
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, auc

# Switch model to evaluation mode
model2.eval()
with torch.no_grad():
    # Retrieve the test data
    test_edge_index = test_data['DSI', 'has_therapeutic_class', 'TC'].edge_label_index
    test_edge_label = test_data['DSI', 'has_therapeutic_class', 'TC'].edge_label
    print(f'shape of test_edge_index = {test_edge_index.shape}')
    print(f'shape of test_edge_label = {test_edge_label.shape}')


    # Make predictions using the model
    test_logits = model2.predict(node_embeddings, test_edge_index)
    print(f'test_logits.shape - {test_logits.shape}')

    test_labels = test_edge_label.cpu().numpy()//2


    # Compute ROC-AUC
    auc_score = roc_auc_score(test_labels, test_logits)

    print(f"Test ROC-AUC: {auc_score:.4f}")

    # Calculate ROC Curve
    fpr, tpr, roc_thresholds = roc_curve(test_labels, test_logits)

    # Calculate Precision-Recall Curve
    precision, recall, pr_thresholds = precision_recall_curve(test_labels, test_logits)

    # Calculate the area under the precision-recall curve (PR-AUC)
    pr_auc = auc(recall, precision)
    print(f"Test PR-AUC: {pr_auc:.4f}")

# Plot ROC Curve
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(fpr, tpr, color='b', label=f'ROC curve (AUC = {auc_score:.4f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')

# Plot Precision-Recall Curve
plt.subplot(1, 2, 2)
plt.plot(recall, precision, color='b', label=f'PR curve (AUC = {pr_auc:.4f})')
plt.title('Precision-Recall Curve')
plt.xlabel('Recall')

plt.ylabel('Precision')
plt.legend(loc='lower left')

# Display the plots
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Assuming test_logits and test_labels are defined from your previous code
plt.figure(figsize=(10, 6))
plt.plot(test_logits.cpu().numpy(), label='Test Predictions')
plt.plot(test_labels, label='Test Labels', linestyle='--')  # Use linestyle to differentiate
plt.xlabel('Data Point Index')
plt.ylabel('Value')
plt.title('Test Predictions vs. Test Labels')
plt.legend()
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, precision_recall_curve


# Plot the loss curves
plt.figure(figsize=(10, 5))
plt.plot(Train_loss, label='Train Loss')
plt.plot(Val_loss, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

"""## Training Loop (Experimentation test)"""

########################################################################################################
#Calculating all node embeddings during message passing
########################################################################################################





import torch.nn.functional as F

# Initialize the model, optimizer, and loss function
model2 = EdgePredictor2(in_channels=data['DSI'].x.shape[1], hidden_channels=12) #Change number of hidden channels (i.e. hidden layers)
optimizer = torch.optim.Adam(model2.parameters(), lr=0.01)
criterion = torch.nn.BCEWithLogitsLoss()
Train_loss = []
Val_loss = []
different_means_of_predictions = []



previous_node_embeddings = None
# Training loop
for epoch in range(1000):  # Adjust epochs as needed
    model2.train()
    optimizer.zero_grad()
        # Accessing node features for each node type


    fraction = 1
    # Randomly sample a fraction of the nodes for each type
    symptom_nodes = train_data['Symptom'].x[torch.randperm(train_data['Symptom'].num_nodes)[:int(fraction * train_data['Symptom'].num_nodes)]]
    disease_nodes = train_data['Disease'].x[torch.randperm(train_data['Disease'].num_nodes)[:int(fraction * train_data['Disease'].num_nodes)]]
    dsi_nodes = train_data['DSI'].x[torch.randperm(train_data['DSI'].num_nodes)[:int(fraction * train_data['DSI'].num_nodes)]]
    tc_nodes = train_data['TC'].x[torch.randperm(train_data['TC'].num_nodes)[:int(fraction * train_data['TC'].num_nodes)]]
    drug_nodes = train_data['Drug'].x[torch.randperm(train_data['Drug'].num_nodes)[:int(fraction * train_data['Drug'].num_nodes)]]

    # Forward pass on train_data


    edge_index = train_data['DSI', 'has_therapeutic_class', 'TC'].edge_index
    edge_label_index = train_data['DSI', 'has_therapeutic_class', 'TC'].edge_label_index
    edge_label = (train_data['DSI', 'has_therapeutic_class', 'TC'].edge_label)//2 #this is your ground truth


######################### do some sampling on the edges ##############################

    # Randomly sample 80% of the edges
#    num_edges = edge_label_index.size(1)
#    sample_size = int(1 * num_edges)
#    perm = torch.randperm(num_edges)[:sample_size]

    # Select the sampled edges and labels
#    edge_index = edge_index[:, perm]
#    edge_label_index = edge_label_index[:, perm]
#    edge_label = edge_label[perm]

####################################################################################

    #updatign nodes so that our algorithm actually learns something
    if previous_node_embeddings is not None:
        x = previous_node_embeddings  # Use the embeddings from the previous iteration
    else:
        x = concat_nodes # Node features only starting here (just in case the edge is read DC --> TC although it should technically not matter?)

    # Compute node embeddings and predict edge probabilities
    node_embeddings = model2(x, edge_index)
    previous_node_embeddings = node_embeddings.detach()

    #print(f'shape of node_embeddings = {node_embeddings.shape}')

    edge_predictions = model2.predict(node_embeddings, edge_label_index)
    tensor_mean = torch.mean(edge_predictions)
    binary_prediciton_tensor = torch.where(edge_predictions > tensor_mean, 1, 0) #We can investigate different thresholds
    different_means_of_predictions.append(tensor_mean)



    # Compute loss and backpropagate
    loss = F.binary_cross_entropy(edge_predictions, edge_label.view(-1).float())
    Train_loss.append(loss.item())
    loss.backward()
    optimizer.step()

    # Validation step
    model2.eval()
    with torch.no_grad():
        val_edge_index = val_data['DSI', 'has_therapeutic_class', 'TC'].edge_index
        val_edge_label_index = val_data['DSI', 'has_therapeutic_class', 'TC'].edge_label_index
        val_edge_label = val_data['DSI', 'has_therapeutic_class', 'TC'].edge_label//2

        val_predictions = model2.predict(node_embeddings, val_edge_label_index)
        val_mean = torch.mean(val_predictions)
        val_binary_prediciton_tensor = torch.where(val_predictions > val_mean, 1, 0)

        val_loss =  F.binary_cross_entropy(val_predictions, val_edge_label.view(-1).float())



        Val_loss.append(val_loss.item())


    print(f"Epoch {epoch}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}")

import matplotlib.pyplot as plt
values = torch.tensor(different_means_of_predictions, requires_grad=True).detach().numpy()
# Assuming 'different_means_of_predictions' is a list of means from your training loop
plt.figure(figsize=(10, 6))
plt.plot(values, label='Mean of Predictions')
plt.xlabel('Epoch')
plt.ylabel('Mean')
plt.title('Mean of Predictions at Each Epoch')
plt.legend()
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, auc

# Switch model to evaluation mode
model2.eval()
with torch.no_grad():
    # Retrieve the test data
    test_edge_index = test_data['DSI', 'has_therapeutic_class', 'TC'].edge_label_index
    test_edge_label = test_data['DSI', 'has_therapeutic_class', 'TC'].edge_label
    print(f'shape of test_edge_index = {test_edge_index.shape}')
    print(f'shape of test_edge_label = {test_edge_label.shape}')


    # Make predictions using the model
    test_logits = model2.predict(node_embeddings, test_edge_index)
    print(f'test_logits.shape - {test_logits.shape}')

    test_labels = test_edge_label.cpu().numpy()//2


    # Compute ROC-AUC
    auc_score = roc_auc_score(test_labels, test_logits)

    print(f"Test ROC-AUC: {auc_score:.4f}")

    # Calculate ROC Curve
    fpr, tpr, roc_thresholds = roc_curve(test_labels, test_logits)

    # Calculate Precision-Recall Curve
    precision, recall, pr_thresholds = precision_recall_curve(test_labels, test_logits)

    # Calculate the area under the precision-recall curve (PR-AUC)
    pr_auc = auc(recall, precision)
    print(f"Test PR-AUC: {pr_auc:.4f}")

# Plot ROC Curve
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(fpr, tpr, color='b', label=f'ROC curve (AUC = {auc_score:.4f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')

# Plot Precision-Recall Curve
plt.subplot(1, 2, 2)
plt.plot(recall, precision, color='b', label=f'PR curve (AUC = {pr_auc:.4f})')
plt.title('Precision-Recall Curve')
plt.xlabel('Recall')

plt.ylabel('Precision')
plt.legend(loc='lower left')

# Display the plots
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Assuming test_logits and test_labels are defined from your previous code
plt.figure(figsize=(10, 6))
plt.plot(test_logits.cpu().numpy(), label='Test predictions')
plt.plot(test_labels, label='Test Labels', linestyle='--')  # Use linestyle to differentiate
plt.xlabel('Data Point Index')
plt.ylabel('Value')
plt.title('Test Predictions vs. Test Labels')
plt.legend()
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, precision_recall_curve


# Plot the loss curves
plt.figure(figsize=(10, 5))
plt.plot(Train_loss, label='Train Loss')
plt.plot(Val_loss, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

"""# LLM use for Recommendation"""

import pandas as pd

df_tc_vocab = pd.read_csv('/content/tc_vocab.csv')


sampled_classes = df_tc_vocab["name"].sample(5).tolist()

def generate_prompt(tc_name):
    return f"""Given the therapeutic class "{tc_name}", recommend the most likely medical specialist a patient should visit and explain your choice briefly in 1 sentence."""


# Call GPT for each sampled therapeutic class and collect results
results = []
for tc_name in sampled_classes:
    prompt = generate_prompt(tc_name)
    try:
        response = openai.ChatCompletion.create(
            model="gpt-4-turbo",
            messages=[
                {"role": "system", "content": "You are a helpful assistant specialized in healthcare advice."},
                {"role": "user", "content": prompt}
            ]
        )
        recommendation = response['choices'][0]['message']['content']
        results.append({"Therapeutic Class": tc_name, "Recommendation": recommendation})
    except Exception as e:
        results.append({"Therapeutic Class": tc_name, "Recommendation": f"Error: {str(e)}"})

# Convert results to a dataframe for better visualization
results_df = pd.DataFrame(results)

# Display the results
results_df

